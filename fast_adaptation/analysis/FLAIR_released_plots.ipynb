{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gT7G52ruQFKF",
   "metadata": {
    "id": "gT7G52ruQFKF"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "S2Rng3FjA52O",
   "metadata": {
    "id": "S2Rng3FjA52O"
   },
   "outputs": [],
   "source": [
    "# Names for plots\n",
    "no_flair = 'Driver'\n",
    "flair = 'Driver + FLAIR'\n",
    "no_perturb_no_flair = 'Driver [No Perturb]'\n",
    "no_perturb_flair = 'Driver + FLAIR [No Perturb]'\n",
    "lqr = 'Driver + LQR'\n",
    "no_perturb_lqr = 'Driver +  LQR [No Perturb]'\n",
    "rl = 'Driver +  Residual-TD3'\n",
    "no_perturb_rl = 'Driver +  Residual-TD3 [No Perturb]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "GEaelSqKUh6F",
   "metadata": {
    "id": "GEaelSqKUh6F"
   },
   "outputs": [],
   "source": [
    "# Algorithms order (from bottom to top)\n",
    "algorithms = [no_perturb_flair, no_perturb_no_flair, flair, rl, lqr, no_flair]\n",
    "\n",
    "# Sections order (from left to right)\n",
    "main_sections = [\"Chicane Static\", \"Chicane Dynamic\"]\n",
    "wind_sections = [\"Ramp\", \"Wind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wlOUbntyUgHn",
   "metadata": {
    "id": "wlOUbntyUgHn"
   },
   "outputs": [],
   "source": [
    "# Printing constants\n",
    "subfigure_width = 10\n",
    "row_height = 0.40\n",
    "fontsize = 32\n",
    "title_fontsize = 38\n",
    "max_ticks = 4\n",
    "color_palette='colorblind'\n",
    "\n",
    "params = {\n",
    "    \"axes.labelsize\": fontsize,\n",
    "    \"axes.titlesize\": title_fontsize,\n",
    "    \"legend.fontsize\": fontsize,\n",
    "    \"xtick.labelsize\": fontsize,\n",
    "    \"xtick.major.size\": fontsize,\n",
    "    \"xtick.major.width\": 2,\n",
    "    \"ytick.labelsize\": fontsize,\n",
    "    \"ytick.major.size\": fontsize,\n",
    "    \"ytick.major.width\": 2,\n",
    "    \"text.usetex\": False,\n",
    "    \"axes.titlepad\": fontsize,\n",
    "    \"axes.linewidth\": 4,\n",
    "    \"lines.linewidth\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "jG23NrEMn0_Y",
   "metadata": {
    "id": "jG23NrEMn0_Y"
   },
   "outputs": [],
   "source": [
    "# Error computation parameters\n",
    "# All given in number of command points\n",
    "# NOTE: this are coming from the vicon so appear to be ~ 150 Hz\n",
    "# 400 is roughly 3 seconds, out of which 1/4 is in the past for better synchronisation\n",
    "num_points_before = 100\n",
    "num_points_after = 300\n",
    "buffer_length = num_points_after + num_points_before\n",
    "min_delay = 30\n",
    "max_delay = 150\n",
    "\n",
    "use_denoising = False\n",
    "denoising_before = 10\n",
    "denoising_after = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9nNlu1q9IcSZ",
   "metadata": {
    "id": "9nNlu1q9IcSZ"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dd45b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6dd45b9",
    "outputId": "92e857de-dea9-4681-e5fd-1bd379f42d47"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "from typing import Dict, Tuple\n",
    "import scipy\n",
    "import json\n",
    "from scipy.signal import correlate\n",
    "from scipy.stats import ranksums\n",
    "from rliable import library as rly\n",
    "from rliable import metrics\n",
    "from rliable import plot_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d3564",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9amLC4q3SswI",
   "metadata": {
    "id": "9amLC4q3SswI"
   },
   "source": [
    "### Functions - Load datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f088a9",
   "metadata": {
    "id": "00f088a9"
   },
   "outputs": [],
   "source": [
    "def process_data(path):\n",
    "    full_df = pd.read_csv(path, low_memory=False)\n",
    "    full_df['ty'] = -full_df['ty']\n",
    "    full_df['tx'] = -full_df['tx']\n",
    "\n",
    "    full_df['target_ty'] = -full_df['target_ty']\n",
    "    full_df['target_tx'] = -full_df['target_tx']\n",
    "\n",
    "    print(full_df.columns)\n",
    "    df = full_df[[\n",
    "      'Timesteps',\n",
    "      'Reps',\n",
    "      'Sections_start_time',\n",
    "      'Sections_end_time',\n",
    "      'Sections',\n",
    "      'Sections_index',\n",
    "      'tx',\n",
    "      'ty',\n",
    "      'index',\n",
    "      'target_tx',\n",
    "      'target_ty',\n",
    "      \"Time\",\n",
    "      \"Damage_Type\",\n",
    "      \"Scaling_Value\",\n",
    "      'human_cmd_lin_x',\n",
    "      'human_cmd_ang_z',\n",
    "      'vx',\n",
    "      'wz',\n",
    "      'adaptation_cmd_ang_z',\n",
    "      'adaptation_cmd_lin_x',\n",
    "      'adaptation_end',\n",
    "    ]]\n",
    "\n",
    "    df = df.sort_values('Time')\n",
    "\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format=\"mixed\")\n",
    "\n",
    "    df[\"TimeDelta\"] = df[\"Time\"].diff().fillna(pd.Timedelta(0.0))\n",
    "    df[\"TimeDelta\"] = df[\"TimeDelta\"].apply(lambda x: x.value / 10**9)\n",
    "\n",
    "    df = df.drop(df[~((df['TimeDelta'] < 0.5) & (df['TimeDelta'] > 0.0))].index)\n",
    "\n",
    "    df = df.set_index(['Reps','Sections','Sections_index']).fillna(method='bfill').fillna(method='ffill')\n",
    "    df['targets'] = pd.Categorical(df[['target_tx','target_ty']].apply(lambda x: tuple(x),axis=1),ordered=True).codes\n",
    "\n",
    "#     df = df.drop(columns=['index','target_tx','target_ty']).drop_duplicates()\n",
    "    df = df.drop(columns=['index']).drop_duplicates()\n",
    "\n",
    "    timings = df.reset_index().groupby(['Reps','Sections','Sections_index','targets'])['TimeDelta'].cumsum()\n",
    "\n",
    "    df.loc[:,'timings'] = timings.values\n",
    "\n",
    "    return df,full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09uV26sF208F",
   "metadata": {
    "id": "09uV26sF208F"
   },
   "source": [
    "### Functions - Common Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "SFaXOA1c2yt8",
   "metadata": {
    "id": "SFaXOA1c2yt8"
   },
   "outputs": [],
   "source": [
    "# Naming functions\n",
    "def naming(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "  dataframe = dataframe.reset_index()\n",
    "\n",
    "  # Filtering based on names\n",
    "  def filter_condition(row):\n",
    "      if '_ramp_only' in row['Reps']:\n",
    "          return row['Sections'] != 'Wind section'\n",
    "      if '_wind_only' in row['Reps']:\n",
    "          return row['Sections'] != 'Ramp section'\n",
    "      return True\n",
    "\n",
    "  # Apply the filtering condition\n",
    "  dataframe = dataframe[dataframe.apply(filter_condition, axis=1)]\n",
    "\n",
    "  # Renaming of algorithms\n",
    "  dataframe['Algorithms_naming'] = dataframe['Reps'].apply(lambda x:\n",
    "      no_perturb_no_flair if (\"NO\" in x and \"OFF\" in x)\n",
    "      else (\n",
    "          no_flair if \"OFF\" in x else (\n",
    "              no_perturb_flair if \"NO\" in x\n",
    "              else flair\n",
    "          )\n",
    "      )\n",
    "  )\n",
    "  # Renaming of section\n",
    "  dataframe['Sections_naming'] = dataframe['Sections'].apply(lambda x:\n",
    "      \"Wind\" if x == \"Wind section\" else (\"Ramp\" if x == \"Ramp section\" else x)\n",
    "  )\n",
    "  dataframe['Sections_naming'] = dataframe[['Sections_naming', 'Damage_Type']].apply(lambda x:\n",
    "      (x[0] + \" Dynamic\") if (x[1] == \"dynamic_value_scaling\") else ((x[0] + \" Static\") if (x[1] == \"static_scaling\" and x[0] != \"Ramp\") else x[0]),\n",
    "      axis=1,\n",
    "  )\n",
    "\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "MKoe1Cch54qu",
   "metadata": {
    "id": "MKoe1Cch54qu"
   },
   "outputs": [],
   "source": [
    "# Dictionary functions\n",
    "def results_as_dict(dataframe: pd.DataFrame, metric_name: str) -> Dict:\n",
    "\n",
    "  # Create main dictionary\n",
    "  all_values = {}\n",
    "  for section in dataframe[\"Sections_naming\"].drop_duplicates().values:\n",
    "    sub_dataframe = dataframe[dataframe[\"Sections_naming\"] == section]\n",
    "    values = {}\n",
    "    for naming in sub_dataframe[\"Algorithms_naming\"].drop_duplicates().values:\n",
    "      values[naming] = np.expand_dims(sub_dataframe[sub_dataframe[\"Algorithms_naming\"] == naming][metric_name].values, axis=1)\n",
    "    all_values[section] = values\n",
    "\n",
    "  # Copy Chicane Baseline twice for Dynamic and Static Chicane\n",
    "  final_values = {}\n",
    "  for section in all_values.keys():\n",
    "    if section == \"Chicane Dynamic\":\n",
    "      final_values[section] = {**all_values[section], **all_values[\"Chicane\"]}\n",
    "    elif section == \"Chicane Static\":\n",
    "      final_values[section] = {**all_values[section], **all_values[\"Chicane\"]}\n",
    "    elif section != \"Chicane\":\n",
    "      final_values[section] = all_values[section]\n",
    "\n",
    "  return final_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ivPRgti95-FR",
   "metadata": {
    "id": "ivPRgti95-FR"
   },
   "outputs": [],
   "source": [
    "# Aggregate functions\n",
    "def results_as_aggregates_scipy(final_values: Dict) -> Tuple[Dict, Dict]:\n",
    "\n",
    "  # Compute intervals\n",
    "  all_aggregate_values = {}\n",
    "  all_aggregate_values_cis = {}\n",
    "  for section in final_values.keys():\n",
    "\n",
    "    values = final_values[section]\n",
    "\n",
    "    # Get aggregates using scipy\n",
    "    aggregate_values = {}\n",
    "    aggregate_values_cis = {}\n",
    "    for algo in values.keys():\n",
    "      aggregate_values[algo] = scipy.stats.trim_mean(values[algo].squeeze(), proportiontocut=0.25, axis=None)\n",
    "      aggregate_values_cis[algo] = scipy.stats.mstats.mquantiles(values[algo].squeeze(), prob=[0.25, 0.75])\n",
    "\n",
    "    all_aggregate_values[section] = aggregate_values\n",
    "    all_aggregate_values_cis[section] = aggregate_values_cis\n",
    "\n",
    "  return all_aggregate_values, all_aggregate_values_cis\n",
    "\n",
    "\n",
    "def results_as_aggregates_rliable(final_values: Dict) -> Tuple[Dict, Dict]:\n",
    "\n",
    "  # Compute intervals\n",
    "  all_aggregate_values = {}\n",
    "  all_aggregate_values_cis = {}\n",
    "  for section in final_values.keys():\n",
    "\n",
    "    values = final_values[section]\n",
    "\n",
    "    # Get aggregates using rliable\n",
    "    aggregate_func = lambda x: np.array([\n",
    "      # metrics.aggregate_median(x),\n",
    "      metrics.aggregate_iqm(x),\n",
    "      # metrics.aggregate_mean(x),\n",
    "      # metrics.aggregate_optimality_gap(x),\n",
    "    ])\n",
    "    aggregate_values, aggregate_values_cis = rly.get_interval_estimates(\n",
    "      values, aggregate_func,\n",
    "    )\n",
    "\n",
    "    all_aggregate_values[section] = aggregate_values\n",
    "    all_aggregate_values_cis[section] = aggregate_values_cis\n",
    "\n",
    "  return all_aggregate_values, all_aggregate_values_cis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedd129",
   "metadata": {},
   "source": [
    "### Functions - Error dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qEmVznTHmXnA",
   "metadata": {
    "id": "qEmVznTHmXnA"
   },
   "outputs": [],
   "source": [
    "def create_error_dataframe(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "  error_dataframe = pd.DataFrame()\n",
    "  total_number_section = len(dataframe[\"Sections_index\"].drop_duplicates().values)\n",
    "  for section_index in dataframe[\"Sections_index\"].drop_duplicates().values:\n",
    "\n",
    "      sub_df = dataframe[dataframe[\"Sections_index\"] == section_index]\n",
    "\n",
    "      # Get the command and sensor\n",
    "      command_df = sub_df[sub_df[\"human_cmd_lin_x\"] == sub_df[\"human_cmd_lin_x\"]]\n",
    "      sensor_df = sub_df[sub_df[\"vx\"] == sub_df[\"vx\"]]\n",
    "\n",
    "      # Extract the buffers\n",
    "      sensor_time = np.asarray(sensor_df[\"Time\"].values)\n",
    "      sensor_x_buffer = np.asarray(sensor_df[\"vx\"].values)\n",
    "      sensor_z_buffer = np.asarray(-sensor_df[\"wz\"].values * np.pi / 180)\n",
    "      command_time = np.asarray(command_df[\"Time\"].values)\n",
    "      command_x_buffer = np.asarray(command_df[\"human_cmd_lin_x\"].values)\n",
    "      command_z_buffer = np.asarray(command_df[\"human_cmd_ang_z\"].values)\n",
    "\n",
    "      # For each command point, find matching sensor point\n",
    "      total_length = command_df.shape[0]\n",
    "      vicon_vx = np.zeros((total_length))\n",
    "      vicon_wz = np.zeros((total_length))\n",
    "      sensor_line = 0\n",
    "      previous_index_delay_x = 0\n",
    "      previous_index_delay_z = 0\n",
    "      for line in range(num_points_before, total_length - num_points_after):\n",
    "\n",
    "        # Get the buffer_length command points\n",
    "        sub_command_time = command_time[line - num_points_before:line + num_points_after]\n",
    "        sub_command_x_buffer = command_x_buffer[line - num_points_before:line + num_points_after]\n",
    "        sub_command_z_buffer = command_z_buffer[line - num_points_before:line + num_points_after]\n",
    "        index_sensor_1 = np.where(sensor_time >= sub_command_time[0])[0][0]\n",
    "        index_sensor_2 = np.where(sensor_time <= sub_command_time[-1])[0][-1]\n",
    "        sub_sensor_time = sensor_time[index_sensor_1:index_sensor_2]\n",
    "        sub_sensor_x_buffer = sensor_x_buffer[index_sensor_1:index_sensor_2]\n",
    "        sub_sensor_z_buffer = sensor_z_buffer[index_sensor_1:index_sensor_2]\n",
    "\n",
    "        # Compute the matching using correlation\n",
    "        cross_correlation_x = correlate(sub_command_x_buffer, sub_sensor_x_buffer)\n",
    "        index_delay_x = np.argmax(cross_correlation_x) - buffer_length\n",
    "        index_delay_x = min(-min_delay, max(index_delay_x, -max_delay))\n",
    "\n",
    "        cross_correlation_z = correlate(sub_command_z_buffer, sub_sensor_z_buffer)\n",
    "        index_delay_z = np.argmax(cross_correlation_z) - buffer_length\n",
    "        index_delay_z = min(-min_delay, max(index_delay_z, -max_delay))\n",
    "\n",
    "        # Just do some quick plotting to debug\n",
    "        # if index_delay_x != previous_index_delay_x and index_delay_z != previous_index_delay_z:\n",
    "        #   print(\"\\n\")\n",
    "        #   print(index_delay_x, index_delay_z)\n",
    "        #   fig, ax = plt.subplots(2, 2,figsize=(38, 6))\n",
    "        #   ax[0, 0].plot(sub_command_x_buffer, label=\"Command x\")\n",
    "        #   ax[0, 0].plot(sub_sensor_x_buffer, label=\"Sensor x\")\n",
    "        #   ax[0, 0].plot(sub_sensor_x_buffer[- index_delay_x:], label=\"Synchronised Sensor x\")\n",
    "        #   ax[0, 0].legend()\n",
    "        #   ax[0, 1].plot(cross_correlation_x, label=\"Crosscorelation x\")\n",
    "        #   ax[0, 1].axvline(np.argmax(cross_correlation_x))\n",
    "        #   ax[1, 0].plot(sub_command_z_buffer, label=\"Command x\")\n",
    "        #   ax[1, 0].plot(sub_sensor_z_buffer, label=\"Sensor x\")\n",
    "        #   ax[1, 0].plot(sub_sensor_z_buffer[- index_delay_z:], label=\"Synchronised Sensor y\")\n",
    "        #   ax[1, 0].legend()\n",
    "        #   ax[1, 1].plot(cross_correlation_z, label=\"Crosscorelation x\")\n",
    "        #   ax[1, 1].axvline(np.argmax(cross_correlation_z))\n",
    "        #   plt.show()\n",
    "        #   previous_index_delay_x = index_delay_x\n",
    "        #   previous_index_delay_z = index_delay_z\n",
    "\n",
    "        # Denoise\n",
    "        if use_denoising:\n",
    "          options_x = sub_sensor_x_buffer[num_points_before - index_delay_x - denoising_before: num_points_before - index_delay_x + denoising_after]\n",
    "          options_z = sub_sensor_z_buffer[num_points_before - index_delay_z - denoising_before: num_points_before - index_delay_z + denoising_after]\n",
    "\n",
    "          vicon_vx[line] = np.mean(options_x)\n",
    "          vicon_wz[line] = np.mean(options_z)\n",
    "\n",
    "        else:\n",
    "          vicon_vx[line] = sub_sensor_x_buffer[num_points_before - index_delay_x]\n",
    "          vicon_wz[line] = sub_sensor_z_buffer[num_points_before - index_delay_z]\n",
    "\n",
    "      # Create the new dataframe\n",
    "      new_error_dataframe = pd.DataFrame.from_dict(\n",
    "          {\n",
    "              \"Timesteps\": command_df[\"Timesteps\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"targets\": command_df[\"targets\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"target_tx\": command_df[\"target_tx\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"target_ty\": command_df[\"target_ty\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"tx\":command_df[\"tx\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"ty\":command_df[\"ty\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"Reps\": command_df[\"Reps\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"Sections_start_time\": command_df[\"Sections_start_time\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"Sections_end_time\": command_df[\"Sections_end_time\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"Scaling_Value\": command_df[\"Scaling_Value\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"Damage_Type\": command_df[\"Damage_Type\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"Sections\": command_df[\"Sections\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"Sections_index\": command_df[\"Sections_index\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"human_cmd_lin_x\": command_df[\"human_cmd_lin_x\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"human_cmd_ang_z\": command_df[\"human_cmd_ang_z\"].values[num_points_before:total_length - num_points_after],\n",
    "              \"vicon_vx\": vicon_vx[num_points_before:total_length - num_points_after],\n",
    "              \"vicon_wz\": vicon_wz[num_points_before:total_length - num_points_after],\n",
    "          }\n",
    "      )\n",
    "      error_dataframe = pd.concat(\n",
    "          [error_dataframe, new_error_dataframe], ignore_index=True\n",
    "      )\n",
    "      print(f\"Done with section {section_index} / {total_number_section}.\")\n",
    "\n",
    "  return error_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab078ee6",
   "metadata": {},
   "source": [
    "## Main datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mooyhIUktzsM",
   "metadata": {
    "id": "mooyhIUktzsM"
   },
   "source": [
    "### Load main datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w3GP_jwlN9Tz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3GP_jwlN9Tz",
    "outputId": "bb11edbf-fc00-4e8a-bea6-67ff991fb14b"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df, raw_data = process_data(\"main_comparison.csv\")\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f2516",
   "metadata": {},
   "source": [
    "### Create main dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WZlXJVYEOLRt",
   "metadata": {
    "collapsed": true,
    "id": "WZlXJVYEOLRt"
   },
   "outputs": [],
   "source": [
    "# Get timings\n",
    "time_full = df.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','targets','Scaling_Value'])[['timings','Damage_Type']].last()\n",
    "\n",
    "# Sum all targets timings to get total section times\n",
    "time_full = time_full.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','Scaling_Value','Damage_Type']).sum()\n",
    "time_full = time_full.reset_index()\n",
    "\n",
    "# Add naming\n",
    "time_full = naming(time_full)\n",
    "\n",
    "# Get dictionaries\n",
    "final_times = results_as_dict(time_full, \"timings\")\n",
    "all_aggregate_times, all_aggregate_times_cis = results_as_aggregates_scipy(final_times)\n",
    "all_rliable_times, all_rliable_times_cis = results_as_aggregates_rliable(final_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3sKBgtzmXnA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "c3sKBgtzmXnA",
    "outputId": "385582b9-5204-428d-f861-c1a761811146"
   },
   "outputs": [],
   "source": [
    "dataframe = df.reset_index()\n",
    "error_dataframe = create_error_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14yplQhOS39P",
   "metadata": {
    "id": "14yplQhOS39P"
   },
   "outputs": [],
   "source": [
    "# Compute error\n",
    "error_full = error_dataframe.copy()\n",
    "error_full[\"Error\"] = (\n",
    "    (error_full[\"vicon_vx\"] - error_full[\"human_cmd_lin_x\"]).pow(\n",
    "        2\n",
    "    )\n",
    "    + (\n",
    "        error_full[\"vicon_wz\"] - error_full[\"human_cmd_ang_z\"]\n",
    "    ).pow(2)\n",
    ").pow(1 / 2)\n",
    "\n",
    "# Get error\n",
    "error_full = error_full.reset_index().groupby(['Reps', 'Sections_start_time','Sections_end_time', 'Sections', 'Sections_index', 'Scaling_Value', 'Damage_Type'])[['Error']].median() #quantile(0.5)\n",
    "error_full = error_full.reset_index()\n",
    "\n",
    "# Add naming\n",
    "error_full = naming(error_full)\n",
    "\n",
    "# Get dictionaries\n",
    "final_error = results_as_dict(error_full, \"Error\")\n",
    "all_aggregate_errors, all_aggregate_errors_cis = results_as_aggregates_scipy(final_error)\n",
    "all_rliable_errors, all_rliable_errors_cis = results_as_aggregates_rliable(final_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fb6d1",
   "metadata": {},
   "source": [
    "## LQR datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53def3c3",
   "metadata": {},
   "source": [
    "### Load LQR datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lqr, raw_data_lqr = process_data(\"lqr_comparison.csv\")\n",
    "df_lqr = df_lqr.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7340c0",
   "metadata": {},
   "source": [
    "### Create LQR dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82b332d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming functions\n",
    "def naming_lqr(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "  dataframe = dataframe.reset_index()\n",
    "\n",
    "  # Filtering based on names\n",
    "  def filter_condition(row):\n",
    "      if '_ramp_only' in row['Reps']:\n",
    "          return row['Sections'] != 'Wind section'\n",
    "      if '_wind_only' in row['Reps']:\n",
    "          return row['Sections'] != 'Ramp section'\n",
    "      return True\n",
    "\n",
    "  # Apply the filtering condition\n",
    "  dataframe = dataframe[dataframe.apply(filter_condition, axis=1)]\n",
    "\n",
    "  # Renaming of algorithms\n",
    "  dataframe['Algorithms_naming'] = dataframe['Reps'].apply(lambda x:\n",
    "      no_perturb_no_flair if (\"NO\" in x and \"OFF\" in x)\n",
    "      else (\n",
    "          no_flair if \"OFF\" in x else (\n",
    "              no_perturb_lqr if \"NO\" in x\n",
    "              else lqr\n",
    "          )\n",
    "      )\n",
    "  )\n",
    "  # Renaming of section\n",
    "  dataframe['Sections_naming'] = dataframe['Sections'].apply(lambda x:\n",
    "      \"Wind\" if x == \"Wind section\" else (\"Ramp\" if x == \"Ramp section\" else x)\n",
    "  )\n",
    "  dataframe['Sections_naming'] = dataframe[['Sections_naming', 'Damage_Type']].apply(lambda x:\n",
    "      (x[0] + \" Dynamic\") if (x[1] == \"dynamic_value_scaling\") else ((x[0] + \" Static\") if (x[1] == \"static_scaling\" and x[0] != \"Ramp\") else x[0]),\n",
    "      axis=1,\n",
    "  )\n",
    "\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9bda2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary functions\n",
    "def results_as_dict_lqr(dataframe: pd.DataFrame, metric_name: str) -> Dict:\n",
    "\n",
    "  # Create main dictionary\n",
    "  all_values = {}\n",
    "  for section in dataframe[\"Sections_naming\"].drop_duplicates().values:\n",
    "    sub_dataframe = dataframe[dataframe[\"Sections_naming\"] == section]\n",
    "    values = {}\n",
    "    for naming in sub_dataframe[\"Algorithms_naming\"].drop_duplicates().values:\n",
    "      values[naming] = np.expand_dims(sub_dataframe[sub_dataframe[\"Algorithms_naming\"] == naming][metric_name].values, axis=1)\n",
    "    all_values[section] = values\n",
    "\n",
    "  return all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d77bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timings\n",
    "time_lqr = df_lqr.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','targets','Scaling_Value'])[['timings','Damage_Type']].last()\n",
    "\n",
    "# Sum all targets timings to get total section times\n",
    "time_lqr = time_lqr.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','Scaling_Value','Damage_Type']).sum()\n",
    "time_lqr = time_lqr.reset_index()\n",
    "\n",
    "# Add naming\n",
    "time_lqr = naming_lqr(time_lqr)\n",
    "\n",
    "# Get dictionaries\n",
    "final_lqr_times = results_as_dict_lqr(time_lqr, \"timings\")\n",
    "all_aggregate_lqr_times, all_aggregate_lqr_times_cis = results_as_aggregates_scipy(final_lqr_times)\n",
    "all_rliable_lqr_times, all_rliable_lqr_times_cis = results_as_aggregates_rliable(final_lqr_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_lqr.reset_index()\n",
    "error_lqr_dataframe = create_error_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error\n",
    "error_lqr = error_lqr_dataframe.copy()\n",
    "error_lqr[\"Error\"] = (\n",
    "    (error_lqr[\"vicon_vx\"] - error_lqr[\"human_cmd_lin_x\"]).pow(\n",
    "        2\n",
    "    )\n",
    "    + (\n",
    "        error_lqr[\"vicon_wz\"] - error_lqr[\"human_cmd_ang_z\"]\n",
    "    ).pow(2)\n",
    ").pow(1 / 2)\n",
    "\n",
    "# Get error\n",
    "error_lqr = error_lqr.reset_index().groupby(['Reps', 'Sections_start_time','Sections_end_time', 'Sections', 'Sections_index', 'Scaling_Value', 'Damage_Type'])[['Error']].median() #quantile(0.5)\n",
    "error_lqr = error_lqr.reset_index()\n",
    "\n",
    "# Add naming\n",
    "error_lqr = naming_lqr(error_lqr)\n",
    "\n",
    "# Get dictionaries\n",
    "final_lqr_error = results_as_dict_lqr(error_lqr, \"Error\")\n",
    "all_aggregate_lqr_errors, all_aggregate_lqr_errors_cis = results_as_aggregates_scipy(final_lqr_error)\n",
    "all_rliable_lqr_errors, all_rliable_lqr_errors_cis = results_as_aggregates_rliable(final_lqr_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcad2d2",
   "metadata": {},
   "source": [
    "## LQR Robustness datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a5010",
   "metadata": {},
   "source": [
    "### Load LQR Robustness datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_robustness_lqr, raw_data_robustness_lqr = process_data(\"lqr_robustness_comparison.csv\")\n",
    "df_robustness_lqr = df_robustness_lqr.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36762ef0",
   "metadata": {},
   "source": [
    "### Create LQR dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timings\n",
    "time_robustness_lqr = df_robustness_lqr.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','targets','Scaling_Value'])[['timings','Damage_Type']].last()\n",
    "\n",
    "# Sum all targets timings to get total section times\n",
    "time_robustness_lqr = time_robustness_lqr.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','Scaling_Value','Damage_Type']).sum()\n",
    "time_lrobustness_qr = time_robustness_lqr.reset_index()\n",
    "\n",
    "# Add naming\n",
    "time_robustness_lqr = naming_lqr(time_robustness_lqr)\n",
    "\n",
    "# Get dictionaries\n",
    "final_robustness_lqr_times = results_as_dict_lqr(time_robustness_lqr, \"timings\")\n",
    "all_aggregate_robustness_lqr_times, all_aggregate_robustness_lqr_times_cis = results_as_aggregates_scipy(final_robustness_lqr_times)\n",
    "all_rliable_robustness_lqr_times, all_rliable_robustness_lqr_times_cis = results_as_aggregates_rliable(final_robustness_lqr_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ecf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_robustness_lqr.reset_index()\n",
    "error_robustness_lqr_dataframe = create_error_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error\n",
    "error_robustness_lqr = error_robustness_lqr_dataframe.copy()\n",
    "error_robustness_lqr[\"Error\"] = (\n",
    "    (error_robustness_lqr[\"vicon_vx\"] - error_robustness_lqr[\"human_cmd_lin_x\"]).pow(\n",
    "        2\n",
    "    )\n",
    "    + (\n",
    "        error_robustness_lqr[\"vicon_wz\"] - error_robustness_lqr[\"human_cmd_ang_z\"]\n",
    "    ).pow(2)\n",
    ").pow(1 / 2)\n",
    "\n",
    "# Get error\n",
    "error_robustness_lqr = error_robustness_lqr.reset_index().groupby(['Reps', 'Sections_start_time','Sections_end_time', 'Sections', 'Sections_index', 'Scaling_Value', 'Damage_Type'])[['Error']].median() #quantile(0.5)\n",
    "error_robustness_lqr = error_robustness_lqr.reset_index()\n",
    "\n",
    "# Add naming\n",
    "error_robustness_lqr = naming_lqr(error_robustness_lqr)\n",
    "\n",
    "# Get dictionaries\n",
    "final_robustness_lqr_error = results_as_dict_lqr(error_robustness_lqr, \"Error\")\n",
    "all_aggregate_robustness_lqr_errors, all_aggregate_robustness_lqr_errors_cis = results_as_aggregates_scipy(final_robustness_lqr_error)\n",
    "all_rliable_robustness_lqr_errors, all_rliable_robustness_lqr_errors_cis = results_as_aggregates_rliable(final_robustness_lqr_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12b21e",
   "metadata": {},
   "source": [
    "## RL datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d45dc2",
   "metadata": {},
   "source": [
    "### Load RL datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ee199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rl, raw_data_rl = process_data(\"rl_comparison.csv\")\n",
    "df_rl = df_rl.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde99e3",
   "metadata": {},
   "source": [
    "### Create RL dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3960633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming functions\n",
    "def naming_rl(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "  dataframe = dataframe.reset_index()\n",
    "\n",
    "  # Filtering based on names\n",
    "  def filter_condition(row):\n",
    "      if '_ramp_only' in row['Reps']:\n",
    "          return row['Sections'] != 'Wind section'\n",
    "      if '_wind_only' in row['Reps']:\n",
    "          return row['Sections'] != 'Ramp section'\n",
    "      return True\n",
    "\n",
    "  # Apply the filtering condition\n",
    "  dataframe = dataframe[dataframe.apply(filter_condition, axis=1)]\n",
    "\n",
    "  # Renaming of algorithms\n",
    "  dataframe['Algorithms_naming'] = dataframe['Reps'].apply(lambda x:\n",
    "      no_perturb_no_flair if (\"NO\" in x and \"OFF\" in x)\n",
    "      else (\n",
    "          no_flair if \"OFF\" in x else (\n",
    "              no_perturb_rl if \"NO\" in x\n",
    "              else rl\n",
    "          )\n",
    "      )\n",
    "  )\n",
    "  # Renaming of section\n",
    "  dataframe['Sections_naming'] = dataframe['Sections'].apply(lambda x:\n",
    "      \"Wind\" if x == \"Wind section\" else (\"Ramp\" if x == \"Ramp section\" else x)\n",
    "  )\n",
    "  dataframe['Sections_naming'] = dataframe[['Sections_naming', 'Damage_Type']].apply(lambda x:\n",
    "      (x[0] + \" Dynamic\") if (x[1] == \"dynamic_value_scaling\") else ((x[0] + \" Static\") if (x[1] == \"static_scaling\" and x[0] != \"Ramp\") else x[0]),\n",
    "      axis=1,\n",
    "  )\n",
    "\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65a06ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary functions\n",
    "def results_as_dict_rl(dataframe: pd.DataFrame, metric_name: str) -> Dict:\n",
    "\n",
    "  # Create main dictionary\n",
    "  all_values = {}\n",
    "  for section in dataframe[\"Sections_naming\"].drop_duplicates().values:\n",
    "    sub_dataframe = dataframe[dataframe[\"Sections_naming\"] == section]\n",
    "    values = {}\n",
    "    for naming in sub_dataframe[\"Algorithms_naming\"].drop_duplicates().values:\n",
    "      values[naming] = np.expand_dims(sub_dataframe[sub_dataframe[\"Algorithms_naming\"] == naming][metric_name].values, axis=1)\n",
    "    all_values[section] = values\n",
    "\n",
    "  return all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b456385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get timings\n",
    "# time_rl = df_rl.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','targets','Scaling_Value'])[['timings','Damage_Type']].last()\n",
    "\n",
    "# # Sum all targets timings to get total section times\n",
    "# time_rl = time_rl.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','Scaling_Value','Damage_Type']).sum()\n",
    "# time_rl = time_rl.reset_index()\n",
    "\n",
    "# # Add naming\n",
    "# time_rl = naming_rl(time_rl)\n",
    "\n",
    "# # Get dictionaries\n",
    "# final_rl_times = results_as_dict_rl(time_rl, \"timings\")\n",
    "# all_aggregate_rl_times, all_aggregate_rl_times_cis = results_as_aggregates_scipy(final_rl_times)\n",
    "# all_rliable_rl_times, all_rliable_rl_times_cis = results_as_aggregates_rliable(final_rl_times)\n",
    "\n",
    "# Because RL runs all died, just empty time\n",
    "final_rl_times = {}\n",
    "all_aggregate_rl_times = {}\n",
    "all_aggregate_rl_times_cis = {}\n",
    "all_rliable_rl_times = {}\n",
    "all_rliable_rl_times_cis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_rl.reset_index()\n",
    "error_rl_dataframe = create_error_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error\n",
    "error_rl = error_rl_dataframe.copy()\n",
    "error_rl[\"Error\"] = (\n",
    "    (error_rl[\"vicon_vx\"] - error_rl[\"human_cmd_lin_x\"]).pow(\n",
    "        2\n",
    "    )\n",
    "    + (\n",
    "        error_rl[\"vicon_wz\"] - error_rl[\"human_cmd_ang_z\"]\n",
    "    ).pow(2)\n",
    ").pow(1 / 2)\n",
    "\n",
    "# Get error\n",
    "error_rl = error_rl.reset_index().groupby(['Reps', 'Sections_start_time','Sections_end_time', 'Sections', 'Sections_index', 'Scaling_Value', 'Damage_Type'])[['Error']].median() #quantile(0.5)\n",
    "error_rl = error_rl.reset_index()\n",
    "\n",
    "# Add naming\n",
    "error_rl = naming_rl(error_rl)\n",
    "\n",
    "# Get dictionaries\n",
    "final_rl_error = results_as_dict_rl(error_rl, \"Error\")\n",
    "all_aggregate_rl_errors, all_aggregate_rl_errors_cis = results_as_aggregates_scipy(final_rl_error)\n",
    "all_rliable_rl_errors, all_rliable_rl_errors_cis = results_as_aggregates_rliable(final_rl_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072e0a1",
   "metadata": {},
   "source": [
    "## Wind datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a19fef",
   "metadata": {},
   "source": [
    "### Load wind datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind, raw_data_wind = process_data(\"main_comparison_wind.csv\")\n",
    "df_wind = df_wind.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db877f",
   "metadata": {},
   "source": [
    "### Create wind dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6060887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timings\n",
    "time_wind = df_wind.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','targets','Scaling_Value'])[['timings','Damage_Type']].last()\n",
    "\n",
    "# Sum all targets timings to get total section times\n",
    "time_wind = time_wind.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','Scaling_Value','Damage_Type']).sum()\n",
    "time_wind = time_wind.reset_index()\n",
    "\n",
    "# Add naming\n",
    "time_wind = naming(time_wind)\n",
    "\n",
    "# Get dictionaries\n",
    "final_wind_times = results_as_dict(time_wind, \"timings\")\n",
    "all_aggregate_wind_times, all_aggregate_wind_times_cis = results_as_aggregates_scipy(final_wind_times)\n",
    "all_rliable_wind_times, all_rliable_wind_times_cis = results_as_aggregates_rliable(final_wind_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_wind.reset_index()\n",
    "error_wind_dataframe = create_error_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e770df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error\n",
    "error_wind = error_wind_dataframe.copy()\n",
    "error_wind[\"Error\"] = (\n",
    "    (error_wind[\"vicon_vx\"] - error_wind[\"human_cmd_lin_x\"]).pow(\n",
    "        2\n",
    "    )\n",
    "    + (\n",
    "        error_wind[\"vicon_wz\"] - error_wind[\"human_cmd_ang_z\"]\n",
    "    ).pow(2)\n",
    ").pow(1 / 2)\n",
    "\n",
    "# Get error\n",
    "error_wind = error_wind.reset_index().groupby(['Reps', 'Sections_start_time','Sections_end_time', 'Sections', 'Sections_index', 'Scaling_Value', 'Damage_Type'])[['Error']].median() #quantile(0.5)\n",
    "error_wind = error_wind.reset_index()\n",
    "\n",
    "# Add naming\n",
    "error_wind = naming(error_wind)\n",
    "\n",
    "# Get dictionaries\n",
    "final_wind_error = results_as_dict(error_wind, \"Error\")\n",
    "all_aggregate_wind_errors, all_aggregate_wind_errors_cis = results_as_aggregates_scipy(final_wind_error)\n",
    "all_rliable_wind_errors, all_rliable_wind_errors_cis = results_as_aggregates_rliable(final_wind_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505721a",
   "metadata": {},
   "source": [
    "### Load wind datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RHx-a3w9f0LA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHx-a3w9f0LA",
    "outputId": "2ad96d00-0169-48c2-af07-691425385dce"
   },
   "outputs": [],
   "source": [
    "df_wind, raw_data_wind = process_data(\"main_comparison_wind.csv\")\n",
    "df_wind = df_wind.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c0587",
   "metadata": {},
   "source": [
    "### Create wind dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb026da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timings\n",
    "time_wind = df_wind.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','targets','Scaling_Value'])[['timings','Damage_Type']].last()\n",
    "\n",
    "# Sum all targets timings to get total section times\n",
    "time_wind = time_wind.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','Scaling_Value','Damage_Type']).sum()\n",
    "time_wind = time_wind.reset_index()\n",
    "\n",
    "# Add naming\n",
    "time_wind = naming(time_wind)\n",
    "\n",
    "# Get dictionaries\n",
    "final_wind_times = results_as_dict(time_wind, \"timings\")\n",
    "all_aggregate_wind_times, all_aggregate_wind_times_cis = results_as_aggregates_scipy(final_wind_times)\n",
    "all_rliable_wind_times, all_rliable_wind_times_cis = results_as_aggregates_rliable(final_wind_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_wind.reset_index()\n",
    "error_wind_dataframe = create_error_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error\n",
    "error_wind = error_wind_dataframe.copy()\n",
    "error_wind[\"Error\"] = (\n",
    "    (error_wind[\"vicon_vx\"] - error_wind[\"human_cmd_lin_x\"]).pow(\n",
    "        2\n",
    "    )\n",
    "    + (\n",
    "        error_wind[\"vicon_wz\"] - error_wind[\"human_cmd_ang_z\"]\n",
    "    ).pow(2)\n",
    ").pow(1 / 2)\n",
    "\n",
    "# Get error\n",
    "error_wind = error_wind.reset_index().groupby(['Reps', 'Sections_start_time','Sections_end_time', 'Sections', 'Sections_index', 'Scaling_Value', 'Damage_Type'])[['Error']].median() #quantile(0.5)\n",
    "error_wind = error_wind.reset_index()\n",
    "\n",
    "# Add naming\n",
    "error_wind = naming(error_wind)\n",
    "\n",
    "# Get dictionaries\n",
    "final_wind_error = results_as_dict(error_wind, \"Error\")\n",
    "all_aggregate_wind_errors, all_aggregate_wind_errors_cis = results_as_aggregates_scipy(final_wind_error)\n",
    "all_rliable_wind_errors, all_rliable_wind_errors_cis = results_as_aggregates_rliable(final_wind_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161dadac",
   "metadata": {},
   "source": [
    "## Robustness datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0800d5",
   "metadata": {},
   "source": [
    "### Load robustness datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DM93dUtI9qQ_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM93dUtI9qQ_",
    "outputId": "5d4d1fcc-abaa-44b9-a446-6d730d547d99"
   },
   "outputs": [],
   "source": [
    "robustness_df, robustness_raw_data = process_data(\"robustness_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434e060",
   "metadata": {},
   "source": [
    "### Create robustness dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc_uNRr9Cyp",
   "metadata": {
    "id": "bfc_uNRr9Cyp"
   },
   "outputs": [],
   "source": [
    "# Get timings\n",
    "time_robustness = robustness_df.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','targets','Scaling_Value'])[['timings','Damage_Type']].last()\n",
    "\n",
    "# Sum all targets timings to get total section times\n",
    "time_robustness = time_robustness.reset_index().groupby(['Reps','Sections_start_time','Sections_end_time','Sections','Sections_index','Scaling_Value','Damage_Type']).sum()\n",
    "time_robustness = time_robustness.reset_index()\n",
    "\n",
    "# Add naming\n",
    "time_robustness = naming(time_robustness)\n",
    "\n",
    "# Get dictionaries\n",
    "final_robustness_times = results_as_dict(time_robustness, \"timings\")\n",
    "all_aggregate_robustness_times, all_aggregate_robustness_times_cis = results_as_aggregates_scipy(final_robustness_times)\n",
    "all_rliable_robustness_times, all_rliable_robustness_times_cis = results_as_aggregates_rliable(final_robustness_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1olZAYmXnA",
   "metadata": {
    "collapsed": true,
    "id": "4d1olZAYmXnA"
   },
   "outputs": [],
   "source": [
    "robustness_dataframe = robustness_df.reset_index()\n",
    "error_robustness_dataframe = create_error_dataframe(robustness_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PcgUisrJS39P",
   "metadata": {
    "id": "PcgUisrJS39P"
   },
   "outputs": [],
   "source": [
    "# Robustness\n",
    "\n",
    "# Compute error\n",
    "error_robustness = error_robustness_dataframe.copy()\n",
    "error_robustness[\"Error\"] = (\n",
    "    (error_robustness[\"vicon_vx\"] - error_robustness[\"human_cmd_lin_x\"]).pow(\n",
    "        2\n",
    "    )\n",
    "    + (\n",
    "        error_robustness[\"vicon_wz\"] - error_robustness[\"human_cmd_ang_z\"]\n",
    "    ).pow(2)\n",
    ").pow(1 / 2)\n",
    "\n",
    "# Get error\n",
    "error_robustness = error_robustness.reset_index().groupby(['Reps', 'Sections_start_time','Sections_end_time', 'Sections', 'Sections_index', 'Scaling_Value', 'Damage_Type'])[['Error']].median() #quantile(0.5)\n",
    "error_robustness = error_robustness.reset_index()\n",
    "\n",
    "# Add naming\n",
    "error_robustness = naming(error_robustness)\n",
    "\n",
    "# Get dictionaries\n",
    "final_robustness_error = results_as_dict(error_robustness, \"Error\")\n",
    "all_aggregate_robustness_errors, all_aggregate_robustness_errors_cis = results_as_aggregates_scipy(final_robustness_error)\n",
    "all_rliable_robustness_errors, all_rliable_robustness_errors_cis = results_as_aggregates_rliable(final_robustness_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3471f",
   "metadata": {},
   "source": [
    "## Scaling datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a343f",
   "metadata": {},
   "source": [
    "### Load scaling datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LlcZe9vU-K3U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlcZe9vU-K3U",
    "outputId": "ca9d5f6f-9ed8-4dc0-8fed-0954a8be0474"
   },
   "outputs": [],
   "source": [
    "scaling_df, scaling_raw_data = process_data(\"scaling_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e61e4c",
   "metadata": {},
   "source": [
    "### Create scaling dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aCDAibVXDgYJ",
   "metadata": {
    "id": "aCDAibVXDgYJ"
   },
   "outputs": [],
   "source": [
    "# Compute the timings\n",
    "time_scaling = scaling_df.reset_index().groupby(['Reps','Sections','Sections_index','targets','Scaling_Value'])[['timings','Damage_Type']].last()\n",
    "\n",
    "# Sum all targets timings to get total section times\n",
    "time_scaling = time_scaling.reset_index().groupby(['Reps','Sections','Sections_index','Scaling_Value','Damage_Type']).sum()\n",
    "time_scaling = time_scaling.reset_index()\n",
    "\n",
    "# Add naming\n",
    "time_scaling = naming(time_scaling)\n",
    "\n",
    "# Separate section based on scaling values\n",
    "new_time_scaling = time_scaling.copy()\n",
    "new_time_scaling[\"Sections_naming\"] = new_time_scaling[\"Sections_naming\"] + \" \" + new_time_scaling[\"Scaling_Value\"].astype(str)\n",
    "\n",
    "# Get dictionaries\n",
    "final_scaling_times = results_as_dict(new_time_scaling, \"timings\")\n",
    "all_aggregate_scaling_times, all_aggregate_scaling_times_cis = results_as_aggregates_scipy(final_scaling_times)\n",
    "all_rliable_scaling_times, all_rliable_scaling_times_cis = results_as_aggregates_rliable(final_scaling_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = scaling_df.reset_index()\n",
    "error_scaling_dataframe = create_error_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d675c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error\n",
    "error_scaling = error_scaling_dataframe.copy()\n",
    "error_scaling[\"Error\"] = (\n",
    "    (error_scaling[\"vicon_vx\"] - error_scaling[\"human_cmd_lin_x\"]).pow(\n",
    "        2\n",
    "    )\n",
    "    + (\n",
    "        error_scaling[\"vicon_wz\"] - error_scaling[\"human_cmd_ang_z\"]\n",
    "    ).pow(2)\n",
    ").pow(1 / 2)\n",
    "\n",
    "# Get error\n",
    "error_scaling = error_scaling.reset_index().groupby(['Reps', 'Sections_start_time','Sections_end_time', 'Sections', 'Sections_index', 'Scaling_Value', 'Damage_Type'])[['Error']].median() #quantile(0.5)\n",
    "error_scaling = error_scaling.reset_index()\n",
    "\n",
    "# Add naming\n",
    "error_scaling = naming(error_scaling)\n",
    "\n",
    "# Separate section based on scaling values\n",
    "new_error_scaling = error_scaling.copy()\n",
    "new_error_scaling[\"Sections_naming\"] = new_error_scaling[\"Sections_naming\"] + \" \" + new_error_scaling[\"Scaling_Value\"].astype(str)\n",
    "\n",
    "# Get dictionaries\n",
    "final_scaling_error = results_as_dict(new_error_scaling, \"Error\")\n",
    "all_aggregate_scaling_errors, all_aggregate_scaling_errors_cis = results_as_aggregates_scipy(final_scaling_error)\n",
    "all_rliable_scaling_errors, all_rliable_scaling_errors_cis = results_as_aggregates_rliable(final_scaling_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h2eKu6J3x61W",
   "metadata": {
    "id": "h2eKu6J3x61W"
   },
   "source": [
    "## Main plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vcEHNhxW4Rax",
   "metadata": {
    "id": "vcEHNhxW4Rax"
   },
   "source": [
    "### Plots - Main plots function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "t3YmXM0mST3e",
   "metadata": {
    "id": "t3YmXM0mST3e"
   },
   "outputs": [],
   "source": [
    "mpl.rcParams.update(params)\n",
    "\n",
    "# Create color frame\n",
    "color_palette = sns.color_palette(color_palette, n_colors=len(algorithms))\n",
    "colors = dict(zip(algorithms, color_palette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ke2GdZPpTQMs",
   "metadata": {
    "id": "ke2GdZPpTQMs"
   },
   "outputs": [],
   "source": [
    "# Redifine this function for subplots\n",
    "def plot_bar(\n",
    "    ax,\n",
    "    point_estimates,\n",
    "    interval_estimates,\n",
    "    datapoints,\n",
    "    algorithms,\n",
    "    colors,\n",
    "    max_ticks,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    title,\n",
    "    metric_title,\n",
    "    plot_datapoints = False,\n",
    "  ):\n",
    "\n",
    "  h = 0.6\n",
    "\n",
    "  for alg_idx, algorithm in enumerate(algorithms):\n",
    "\n",
    "    if algorithm not in interval_estimates.keys() or algorithm not in point_estimates.keys():\n",
    "       continue\n",
    "\n",
    "    # Plot interval estimates.\n",
    "    lower, upper = interval_estimates[algorithm]\n",
    "    ax.barh(\n",
    "        y=alg_idx,\n",
    "        width=upper - lower,\n",
    "        height=h,\n",
    "        left=lower,\n",
    "        color=colors[algorithm],\n",
    "        alpha=0.75,\n",
    "        label=algorithm\n",
    "      )\n",
    "\n",
    "    # Plot point estimates.\n",
    "    ax.vlines(\n",
    "        x=point_estimates[algorithm],\n",
    "        ymin=alg_idx - (8 * h / 16),\n",
    "        ymax=alg_idx + (7.98 * h / 16),\n",
    "        label=algorithm,\n",
    "        color='k',\n",
    "        alpha=0.5,\n",
    "        linewidth=3,\n",
    "    )\n",
    "\n",
    "    # Plot datapoints\n",
    "    if plot_datapoints:\n",
    "      algo_datapoints = datapoints[algorithm].squeeze()\n",
    "      y = np.ones_like(algo_datapoints) * alg_idx\n",
    "      ax.scatter(y=y, x=algo_datapoints, marker='o', color=colors[algorithm], s=200, alpha=0.25)\n",
    "\n",
    "  # Title\n",
    "  ax.set_title(title, weight=\"bold\")\n",
    "  ax.set_xlabel(xlabel)\n",
    "\n",
    "  ax.set_yticks(list(range(len(algorithms))))\n",
    "  ax.xaxis.set_major_locator(plt.MaxNLocator(max_ticks))\n",
    "  ax.tick_params(axis='y', which='both', length=0.0)\n",
    "  ax.tick_params(axis='x', which='both', length=6)\n",
    "\n",
    "  # Grid visual\n",
    "  ax.grid(True, axis='x', alpha=0.25)\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  ax.spines['left'].set_visible(False)\n",
    "  ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "  # Deal with ticks and the blank space at the origin\n",
    "  ax.spines['left'].set_position(('outward', 10))\n",
    "  ax.spines['bottom'].set_position(('outward', 10))\n",
    "\n",
    "  # Add ylabel\n",
    "  if ylabel:\n",
    "      ax.set_yticklabels(algorithms)\n",
    "      ax.text(-0.2, 1.0, metric_title, transform=ax.transAxes, fontsize=title_fontsize, weight=\"bold\", horizontalalignment='right', verticalalignment='bottom')\n",
    "  else:\n",
    "      ax.set_yticklabels([])\n",
    "  ax.yaxis.set_tick_params(pad=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4fc26f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(\n",
    "    ax,\n",
    "    point_estimates,\n",
    "    interval_estimates,\n",
    "    datapoints,\n",
    "    algorithms,\n",
    "    colors,\n",
    "    max_ticks,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    title,\n",
    "    metric_title,\n",
    "    plot_datapoints=False,\n",
    "):\n",
    "    h = 0.6\n",
    "\n",
    "    for alg_idx, algorithm in enumerate(algorithms):\n",
    "        print(datapoints)\n",
    "\n",
    "        if algorithm not in datapoints.keys():\n",
    "            continue\n",
    "\n",
    "        # Plot interval estimates using boxplot\n",
    "        algo_datapoints = datapoints[algorithm].squeeze()\n",
    "        ax.boxplot(\n",
    "            algo_datapoints,\n",
    "            vert=False,\n",
    "            positions=[alg_idx],\n",
    "            widths=h,\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor=colors[algorithm], color=colors[algorithm], alpha=0.75),\n",
    "            medianprops=dict(color='k', linewidth=2),\n",
    "            whiskerprops=dict(linewidth=2),\n",
    "            capprops=dict(linewidth=2),\n",
    "            flierprops=dict(marker='o', color=colors[algorithm], alpha=0.5, markersize=10, linestyle='none', linewidth=2),\n",
    "        )\n",
    "\n",
    "        # Plot point estimates as vertical lines\n",
    "        # ax.vlines(\n",
    "        #     x=point_estimates[algorithm],\n",
    "        #     ymin=alg_idx - h / 2,\n",
    "        #     ymax=alg_idx + h / 2,\n",
    "        #     color='k',\n",
    "        #     alpha=0.5,\n",
    "        #     linewidth=3,\n",
    "        # )\n",
    "\n",
    "        # Optionally, scatter plot datapoints\n",
    "        if plot_datapoints:\n",
    "            y = np.ones_like(algo_datapoints) * alg_idx\n",
    "            ax.scatter(y=y, x=algo_datapoints, marker='o', color=colors[algorithm], s=200, alpha=0.25)\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(title, weight=\"bold\")\n",
    "    ax.set_xlabel(xlabel)\n",
    "\n",
    "    # Set y-ticks and labels\n",
    "    ax.set_yticks(list(range(len(algorithms))))\n",
    "    if ylabel:\n",
    "        ax.set_yticklabels(algorithms)\n",
    "        ax.text(\n",
    "            -0.2, 1.0, metric_title, transform=ax.transAxes, fontsize=40, weight=\"bold\",\n",
    "            horizontalalignment='right', verticalalignment='bottom'\n",
    "        )\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    # Set x-ticks and limits\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(max_ticks))\n",
    "    ax.tick_params(axis='y', which='both', length=0.0)\n",
    "    ax.tick_params(axis='x', which='both', length=6)\n",
    "\n",
    "    # Grid visual\n",
    "    ax.grid(True, axis='x', alpha=0.25)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "    # Deal with ticks and the blank space at the origin\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "\n",
    "    ax.yaxis.set_tick_params(pad=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261d3ef",
   "metadata": {},
   "source": [
    "### Main plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55318240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_main_figure(\n",
    "  filename,\n",
    "  all_agg_times,\n",
    "  all_agg_times_cis,\n",
    "  all_agg_lqr_times,\n",
    "  all_agg_lqr_times_cis,\n",
    "  all_agg_rl_times,\n",
    "  all_agg_rl_times_cis,\n",
    "  all_agg_wind_times,\n",
    "  all_agg_wind_times_cis,\n",
    "  all_agg_robustness_times,\n",
    "  all_agg_robustness_times_cis,\n",
    "  all_agg_robustness_lqr_times,\n",
    "  all_agg_robustness_lqr_times_cis,\n",
    "  all_agg_errors,\n",
    "  all_agg_errors_cis,\n",
    "  all_agg_lqr_errors,\n",
    "  all_agg_lqr_errors_cis,\n",
    "  all_agg_rl_errors,\n",
    "  all_agg_rl_errors_cis,\n",
    "  all_agg_wind_errors,\n",
    "  all_agg_wind_errors_cis,\n",
    "  all_agg_robustness_errors,\n",
    "  all_agg_robustness_errors_cis,\n",
    "  all_agg_robustness_lqr_errors,\n",
    "  all_agg_robustness_lqr_errors_cis,\n",
    "  plot_datapoints = False,\n",
    "  plot_as_bar = True,\n",
    "):\n",
    "  # Create figure\n",
    "  figsize = (subfigure_width * 4, row_height * 10 * len(algorithms))\n",
    "  fig, axes = plt.subplots(nrows=3, ncols=5, figsize=figsize, gridspec_kw={'height_ratios': [1, 0.4, 1]})\n",
    "  axes[1, 0].set_visible(False)\n",
    "  axes[1, 1].set_visible(False)\n",
    "  axes[1, 2].set_visible(False)\n",
    "  axes[1, 3].set_visible(False)\n",
    "  axes[1, 4].set_visible(False)\n",
    "\n",
    "  # Ploting time\n",
    "  section_number = 0\n",
    "  for section in main_sections:\n",
    "    \n",
    "    ax = axes[0, section_number]\n",
    "\n",
    "    complete_agg_times = all_agg_times[section]\n",
    "    complete_agg_times_cis = all_agg_times_cis[section]\n",
    "    if section in all_agg_lqr_times.keys():\n",
    "      complete_agg_times = {**complete_agg_times, **all_agg_lqr_times[section]}\n",
    "      complete_agg_times_cis = {**complete_agg_times_cis, **all_agg_lqr_times_cis[section]}\n",
    "    # if section in all_agg_rl_times.keys():\n",
    "    #   complete_agg_times = {**complete_agg_times, **all_agg_rl_times[section]}\n",
    "    #   complete_agg_times_cis = {**complete_agg_times_cis, **all_agg_rl_times_cis[section]} \n",
    "\n",
    "    datapoints = final_times[section]\n",
    "    if section in final_lqr_times.keys():\n",
    "      datapoints = {**datapoints, **final_lqr_times[section]}\n",
    "    # if section in final_rl_times.keys():\n",
    "    #   datapoints = {**datapoints, **final_rl_times[section]}\n",
    "\n",
    "    if plot_as_bar:\n",
    "      plot_bar(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_times,\n",
    "        interval_estimates=complete_agg_times_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Completion Time (s)\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=section,\n",
    "        metric_title=\"A. Completion Time\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    else:\n",
    "      plot_box(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_times,\n",
    "        interval_estimates=complete_agg_times_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Completion Time (s)\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=section,\n",
    "        metric_title=\"A. Completion Time\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    section_number += 1\n",
    "\n",
    "  for section in wind_sections:\n",
    "\n",
    "    ax = axes[0, section_number]\n",
    "\n",
    "    complete_agg_times = all_agg_wind_times[section]\n",
    "    complete_agg_times_cis = all_agg_wind_times_cis[section]\n",
    "    if section in all_agg_lqr_times.keys():\n",
    "      complete_agg_times = {**complete_agg_times, **all_agg_lqr_times[section]}\n",
    "      complete_agg_times_cis = {**complete_agg_times_cis, **all_agg_lqr_times_cis[section]}\n",
    "    # if section in all_agg_rl_times.keys():\n",
    "    #   complete_agg_times = {**complete_agg_times, **all_agg_rl_times[section]}\n",
    "    #   complete_agg_times_cis = {**complete_agg_times_cis, **all_agg_rl_times_cis[section]}\n",
    "    \n",
    "    datapoints = final_wind_times[section]\n",
    "    if section in final_lqr_times.keys():\n",
    "      datapoints = {**datapoints, **final_lqr_times[section]}\n",
    "    # if section in final_rl_times.keys():\n",
    "    #   datapoints = {**datapoints, **final_rl_times[section]}\n",
    "\n",
    "    if plot_as_bar:\n",
    "      plot_bar(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_times,\n",
    "        interval_estimates=complete_agg_times_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Completion Time (s)\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=section,\n",
    "        metric_title=\"A. Completion Time\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    else:\n",
    "      plot_box(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_times,\n",
    "        interval_estimates=complete_agg_times_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Completion Time (s)\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=section,\n",
    "        metric_title=\"A. Completion Time\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    section_number += 1\n",
    "\n",
    "  section = \"Ramp\"\n",
    "  ax = axes[0, section_number]\n",
    " \n",
    "  complete_agg_times = all_agg_robustness_times[section]\n",
    "  complete_agg_times_cis = all_agg_robustness_times_cis[section]\n",
    "  if section in all_agg_robustness_lqr_times.keys():\n",
    "    complete_agg_times = {**complete_agg_times, **all_agg_robustness_lqr_times[section]}\n",
    "    complete_agg_times_cis = {**complete_agg_times_cis, **all_agg_robustness_lqr_times_cis[section]}\n",
    "\n",
    "  datapoints = final_robustness_times[section]\n",
    "  if section in final_lqr_times.keys():\n",
    "    datapoints = {**datapoints, **final_robustness_lqr_times[section]}\n",
    "\n",
    "  if plot_as_bar:\n",
    "      plot_bar(\n",
    "      ax,\n",
    "      point_estimates=complete_agg_times,\n",
    "      interval_estimates=complete_agg_times_cis,\n",
    "      datapoints=datapoints,\n",
    "      algorithms=algorithms,\n",
    "      colors=colors,\n",
    "      max_ticks=max_ticks,\n",
    "      xlabel=\"Completion Time (s)\",\n",
    "      ylabel=False,\n",
    "      title=\"Robustness\",\n",
    "      metric_title=\"A. Completion Time\",\n",
    "      plot_datapoints=plot_datapoints,\n",
    "    )\n",
    "  else:\n",
    "    plot_box(\n",
    "      ax,\n",
    "      point_estimates=complete_agg_times,\n",
    "      interval_estimates=complete_agg_times_cis,\n",
    "      datapoints=datapoints,\n",
    "      algorithms=algorithms,\n",
    "      colors=colors,\n",
    "      max_ticks=max_ticks,\n",
    "      xlabel=\"Completion Time (s)\",\n",
    "      ylabel=False,\n",
    "      title=\"Robustness\",\n",
    "      metric_title=\"A. Completion Time\",\n",
    "      plot_datapoints=plot_datapoints,\n",
    "    )\n",
    "\n",
    "\n",
    "  # Ploting error\n",
    "  section_number = 0\n",
    "  for section in main_sections:\n",
    "\n",
    "    ax = axes[2, section_number]\n",
    "\n",
    "    complete_agg_errors = all_agg_errors[section]\n",
    "    complete_agg_errors_cis = all_agg_errors_cis[section]\n",
    "    if section in all_agg_lqr_times.keys():\n",
    "      complete_agg_errors = {**complete_agg_errors, **all_agg_lqr_errors[section]}\n",
    "      complete_agg_errors_cis = {**complete_agg_errors_cis, **all_agg_lqr_errors_cis[section]}\n",
    "    # if section in all_agg_rl_times.keys():\n",
    "    #   complete_agg_errors = {**complete_agg_errors, **all_agg_rl_errors[section]}\n",
    "    #   complete_agg_errors_cis = {**complete_agg_errors_cis, **all_agg_rl_errors_cis[section]}\n",
    "    \n",
    "    datapoints = final_error[section]\n",
    "    if section in final_lqr_error.keys():\n",
    "      datapoints = {**datapoints, **final_lqr_error[section]}\n",
    "    # if section in final_rl_error.keys():\n",
    "    #   datapoints = {**datapoints, **final_rl_error[section]}\n",
    "      \n",
    "\n",
    "    if plot_as_bar:\n",
    "      plot_bar(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_errors,\n",
    "        interval_estimates=complete_agg_errors_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Tracking Error\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=None,\n",
    "        metric_title=\"B. Tracking Error\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    else:\n",
    "      plot_box(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_errors,\n",
    "        interval_estimates=complete_agg_errors_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Tracking Error\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=None,\n",
    "        metric_title=\"B. Tracking Error\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    section_number += 1\n",
    "\n",
    "  for section in wind_sections:\n",
    "\n",
    "    ax = axes[2, section_number]\n",
    "\n",
    "    complete_agg_errors = all_agg_wind_errors[section]\n",
    "    complete_agg_errors_cis = all_agg_wind_errors_cis[section]\n",
    "    if section in all_agg_lqr_errors.keys():\n",
    "      complete_agg_errors = {**complete_agg_errors, **all_agg_lqr_errors[section]}\n",
    "      complete_agg_errors_cis = {**complete_agg_errors_cis, **all_agg_lqr_errors_cis[section]}\n",
    "    # if section in all_agg_rl_errors.keys():\n",
    "    #   complete_agg_errors = {**complete_agg_errors, **all_agg_rl_errors[section]}\n",
    "    #   complete_agg_errors_cis = {**complete_agg_errors_cis, **all_agg_rl_errors_cis[section]}\n",
    "    \n",
    "    datapoints = final_wind_error[section]\n",
    "    if section in final_lqr_error.keys():\n",
    "      datapoints = {**datapoints, **final_lqr_error[section]}\n",
    "    # if section in final_rl_error.keys():\n",
    "    #   datapoints = {**datapoints, **final_rl_error[section]}\n",
    "    \n",
    "\n",
    "    if plot_as_bar:\n",
    "      plot_bar(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_errors,\n",
    "        interval_estimates=complete_agg_errors_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Tracking Error\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=None,\n",
    "        metric_title=\"B. Tracking Error\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    else:\n",
    "      plot_box(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_errors,\n",
    "        interval_estimates=complete_agg_errors_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Tracking Error\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=None,\n",
    "        metric_title=\"B. Tracking Error\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "    )\n",
    "    section_number += 1\n",
    "\n",
    "  section = \"Ramp\"\n",
    "  ax = axes[2, section_number]\n",
    "  \n",
    "  complete_agg_errors = all_agg_robustness_errors[section]\n",
    "  complete_agg_errors_cis = all_agg_robustness_errors_cis[section]\n",
    "  if section in all_agg_robustness_lqr_errors.keys():\n",
    "    complete_agg_errors = {**complete_agg_errors, **all_agg_robustness_lqr_errors[section]}\n",
    "    complete_agg_errors_cis = {**complete_agg_errors_cis, **all_agg_robustness_lqr_errors_cis[section]}\n",
    "\n",
    "  datapoints = final_robustness_error[section]\n",
    "  if section in final_robustness_lqr_error.keys():\n",
    "    datapoints = {**datapoints, **final_robustness_lqr_error[section]}\n",
    "\n",
    "  if plot_as_bar:\n",
    "      plot_bar(\n",
    "        ax,\n",
    "        point_estimates=complete_agg_errors,\n",
    "        interval_estimates=complete_agg_errors_cis,\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Tracking Error\",\n",
    "        ylabel=False,\n",
    "        title=None,\n",
    "        metric_title=\"B. Tracking Error\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "  else:\n",
    "    plot_box(\n",
    "      ax,\n",
    "      point_estimates=complete_agg_errors,\n",
    "      interval_estimates=complete_agg_errors_cis,\n",
    "      datapoints=datapoints,\n",
    "      algorithms=algorithms,\n",
    "      colors=colors,\n",
    "      max_ticks=max_ticks,\n",
    "      xlabel=\"Tracking Error\",\n",
    "      ylabel=False,\n",
    "      title=None,\n",
    "      metric_title=\"B. Tracking Error\",\n",
    "      plot_datapoints=plot_datapoints,\n",
    "    )\n",
    "\n",
    "  plt.subplots_adjust(wspace=0.3, hspace=0.05, left=0.0)\n",
    "  plt.show()\n",
    "\n",
    "  fig.savefig(filename, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WrD1JrDPORnz",
   "metadata": {
    "id": "WrD1JrDPORnz"
   },
   "source": [
    "### Plots - Main plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy plots\n",
    "plot_main_figure(\n",
    "  filename='./main_results.svg',\n",
    "  all_agg_times=all_aggregate_times,\n",
    "  all_agg_times_cis=all_aggregate_times_cis,\n",
    "  all_agg_lqr_times=all_aggregate_lqr_times,\n",
    "  all_agg_lqr_times_cis=all_aggregate_lqr_times_cis,\n",
    "  all_agg_rl_times=all_aggregate_rl_times,\n",
    "  all_agg_rl_times_cis=all_aggregate_rl_times_cis,\n",
    "  all_agg_wind_times=all_aggregate_wind_times,\n",
    "  all_agg_wind_times_cis=all_aggregate_wind_times_cis,\n",
    "  all_agg_robustness_times=all_aggregate_robustness_times,\n",
    "  all_agg_robustness_times_cis=all_aggregate_robustness_times_cis,\n",
    "  all_agg_robustness_lqr_times=all_aggregate_robustness_lqr_times,\n",
    "  all_agg_robustness_lqr_times_cis=all_aggregate_robustness_lqr_times_cis,\n",
    "  all_agg_errors=all_aggregate_errors,\n",
    "  all_agg_errors_cis=all_aggregate_errors_cis,\n",
    "  all_agg_lqr_errors=all_aggregate_lqr_errors,\n",
    "  all_agg_lqr_errors_cis=all_aggregate_lqr_errors_cis,\n",
    "  all_agg_rl_errors=all_aggregate_rl_errors,\n",
    "  all_agg_rl_errors_cis=all_aggregate_rl_errors_cis,\n",
    "  all_agg_wind_errors=all_aggregate_wind_errors,\n",
    "  all_agg_wind_errors_cis=all_aggregate_wind_errors_cis,\n",
    "  all_agg_robustness_errors=all_aggregate_robustness_errors,\n",
    "  all_agg_robustness_errors_cis=all_aggregate_robustness_errors_cis,\n",
    "  all_agg_robustness_lqr_errors=all_aggregate_robustness_lqr_errors,\n",
    "  all_agg_robustness_lqr_errors_cis=all_aggregate_robustness_lqr_errors_cis,\n",
    "  plot_datapoints=False,\n",
    "  plot_as_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b44d2",
   "metadata": {},
   "source": [
    "### Scaling Ploting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd32936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scaling_figure(\n",
    "  filename,\n",
    "  all_agg_scaling_times, \n",
    "  all_agg_scaling_times_cis,\n",
    "  all_agg_scaling_errors, \n",
    "  all_agg_scaling_errors_cis,\n",
    "  plot_datapoints = False,\n",
    "  plot_as_bar = True,\n",
    "):\n",
    "\n",
    "  # Create figure\n",
    "  figsize = (subfigure_width * 4, row_height * 10 * len(algorithms))\n",
    "  fig, axes = plt.subplots(nrows=3, ncols=5, figsize=figsize, gridspec_kw={'height_ratios': [1, 0.4, 1]})\n",
    "  axes[1, 0].set_visible(False)\n",
    "  axes[1, 1].set_visible(False)\n",
    "  axes[1, 2].set_visible(False)\n",
    "  axes[1, 3].set_visible(False)\n",
    "  axes[1, 4].set_visible(False)\n",
    "\n",
    "  # Ploting time\n",
    "  section_number = 0\n",
    "  for section in all_agg_scaling_times.keys():\n",
    "\n",
    "    ax = axes[0, section_number]\n",
    "    datapoints = final_scaling_times[section]\n",
    "    if plot_as_bar:\n",
    "      plot_bar(\n",
    "        ax,\n",
    "        point_estimates=all_agg_scaling_times[section],\n",
    "        interval_estimates=all_agg_scaling_times_cis[section],\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Completion Time (s)\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=section,\n",
    "        metric_title=\"A. Completion Time\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    else:\n",
    "      plot_box(\n",
    "        ax,\n",
    "        point_estimates=all_agg_scaling_times[section],\n",
    "        interval_estimates=all_agg_scaling_times_cis[section],\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Completion Time (s)\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=section,\n",
    "        metric_title=\"A. Completion Time\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    section_number += 1\n",
    "\n",
    "  # Ploting error\n",
    "  section_number = 0\n",
    "  for section in all_agg_scaling_times.keys():\n",
    "\n",
    "    ax = axes[2, section_number]\n",
    "    datapoints = final_scaling_error[section]\n",
    "    if plot_as_bar:\n",
    "      plot_bar(\n",
    "        ax,\n",
    "        point_estimates=all_agg_scaling_errors[section],\n",
    "        interval_estimates=all_agg_scaling_errors_cis[section],\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Tracking Error\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=None,\n",
    "        metric_title=\"B. Tracking Error\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    else:\n",
    "      plot_box(\n",
    "        ax,\n",
    "        point_estimates=all_agg_scaling_errors[section],\n",
    "        interval_estimates=all_agg_scaling_errors_cis[section],\n",
    "        datapoints=datapoints,\n",
    "        algorithms=algorithms,\n",
    "        colors=colors,\n",
    "        max_ticks=max_ticks,\n",
    "        xlabel=\"Tracking Error\",\n",
    "        ylabel=(section_number == 0),\n",
    "        title=None,\n",
    "        metric_title=\"B. Tracking Error\",\n",
    "        plot_datapoints=plot_datapoints,\n",
    "      )\n",
    "    section_number += 1\n",
    "\n",
    "  plt.subplots_adjust(wspace=0.3, hspace=0.05, left=0.0)\n",
    "  plt.show()\n",
    "\n",
    "  fig.savefig(filename, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c412d",
   "metadata": {},
   "source": [
    "### Plots - Static Damage plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "436d7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dict(\n",
    "    dict,\n",
    "):\n",
    "    sections = dict.keys()\n",
    "    new_dict = {}\n",
    "    for section in sections:\n",
    "        if section != \"Chicane 1.0\":\n",
    "            new_dict[section] = dict[section]\n",
    "            new_dict[section][no_perturb_flair] = dict[\"Chicane 1.0\"][no_perturb_flair]\n",
    "            new_dict[section][no_perturb_no_flair] = dict[\"Chicane 1.0\"][no_perturb_no_flair]\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc276832",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aggregate_scaling_times = transform_dict(all_aggregate_scaling_times)\n",
    "all_aggregate_scaling_times_cis = transform_dict(all_aggregate_scaling_times_cis)\n",
    "all_aggregate_scaling_errors = transform_dict(all_aggregate_scaling_errors)\n",
    "all_aggregate_scaling_errors_cis = transform_dict(all_aggregate_scaling_errors_cis)\n",
    "final_scaling_error = transform_dict(final_scaling_error)\n",
    "final_scaling_times = transform_dict(final_scaling_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy plots\n",
    "plot_scaling_figure(\n",
    "  filename='./main_scaling_results.svg',\n",
    "  all_agg_scaling_times=all_aggregate_scaling_times, \n",
    "  all_agg_scaling_times_cis=all_aggregate_scaling_times_cis,\n",
    "  all_agg_scaling_errors=all_aggregate_scaling_errors, \n",
    "  all_agg_scaling_errors_cis=all_aggregate_scaling_errors_cis,\n",
    "  plot_datapoints=False,\n",
    "  plot_as_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78daFT73DgX9",
   "metadata": {
    "id": "78daFT73DgX9"
   },
   "source": [
    "### Plots - Static Damage Scaling Laws plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rYBBNxnTDgYJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "rYBBNxnTDgYJ",
    "outputId": "97509aec-cb28-4ac2-b69e-bdc622337a81"
   },
   "outputs": [],
   "source": [
    "baseline_dataframe = time_scaling[time_scaling['Algorithms_naming'] == no_perturb_no_flair]\n",
    "rename_baseline_dataframe = baseline_dataframe.copy()\n",
    "rename_baseline_dataframe['Algorithms_naming'] = no_flair\n",
    "rename_baseline_dataframe['Scaling_Value'] = 1.0\n",
    "flair_baseline_dataframe = time_scaling[time_scaling['Algorithms_naming'] == no_perturb_flair]\n",
    "rename_flair_baseline_dataframe = flair_baseline_dataframe.copy()\n",
    "rename_flair_baseline_dataframe['Algorithms_naming'] = flair\n",
    "rename_flair_baseline_dataframe['Scaling_Value'] = 1.0\n",
    "plotting_dataframe = pd.concat(\n",
    "    [\n",
    "        time_scaling[time_scaling['Algorithms_naming'] == flair],\n",
    "        time_scaling[time_scaling['Algorithms_naming'] == no_flair],\n",
    "        rename_flair_baseline_dataframe,\n",
    "        rename_baseline_dataframe,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "plotting_dataframe[\"Scaling_Value\"] = 1.0 - plotting_dataframe[\"Scaling_Value\"]\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\", n_colors=4)\n",
    "color_baseline = colors[no_perturb_no_flair]\n",
    "color_plot = [colors[flair], colors[no_flair]]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize = (subfigure_width * 6, row_height * 8 * len(algorithms)))\n",
    "\n",
    "# Baseline\n",
    "ax.axhline(baseline_dataframe['timings'].mean(), label=no_perturb_no_flair, c=color_baseline)\n",
    "\n",
    "# Scaling\n",
    "sns.lineplot(\n",
    "    data=plotting_dataframe,\n",
    "    y='timings',\n",
    "    x='Scaling_Value',\n",
    "    hue=\"Algorithms_naming\",\n",
    "    ax=ax,\n",
    "    palette=color_plot,\n",
    ")\n",
    "\n",
    "# Grid visual\n",
    "sns.set_style('whitegrid')\n",
    "ax.grid(True, axis='x', alpha=0.25)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "ax.tick_params(axis='y', which='both', length=0.0)\n",
    "ax.tick_params(axis='x', which='both', length=6)\n",
    "\n",
    "plt.title('Chicane with increasing Perturbation Strength')\n",
    "plt.ylabel('Completion Time (s)')\n",
    "plt.xlabel('Perturbation Strength')\n",
    "\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "plt.legend(bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "fig.savefig('./static_damage_fig.svg', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QXe_s7k4x96b",
   "metadata": {
    "id": "QXe_s7k4x96b"
   },
   "source": [
    "## Trajectories plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BgvZS0eqU9pm",
   "metadata": {
    "id": "BgvZS0eqU9pm"
   },
   "source": [
    "### Trajectories - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "EJ3G1P7JJX0K",
   "metadata": {
    "id": "EJ3G1P7JJX0K"
   },
   "outputs": [],
   "source": [
    "sections = [\"Chicane Dynamic\", \"Chicane Static\", \"Ramp\", \"Wind\"]\n",
    "chosen_trajectories = {\n",
    "  \"Chicane Dynamic\": (\n",
    "      ('NO_perturbation_Adaptation_OFF', 'Chicane', 48.0),\n",
    "      ('Dynamic_Value_0.39999999999999997_0.7_Adaptation_ON', 'Chicane', 13.0),\n",
    "      ('Dynamic_Value_0.39999999999999997_0.7_Adaptation_OFF', 'Chicane', 68.0)\n",
    "  ),\n",
    "  \"Chicane Static\": (\n",
    "      ('NO_perturbation_Adaptation_OFF', 'Chicane', 48.0),\n",
    "      ('Static_0.6_Adaptation_ON', 'Chicane', 86.0),\n",
    "      ('Static_0.6_Adaptation_OFF', 'Chicane', 103.0)\n",
    "  ),\n",
    "  \"Ramp\": (\n",
    "      ('NO_perturbation_Adaptation_OFF', 'Ramp section', 100.0),\n",
    "      ('Static_0.7_Adaptation_ON', 'Ramp section', 26.0),\n",
    "      ('Static_0.7_Adaptation_OFF', 'Ramp section', 138.0)\n",
    "  ),\n",
    "  \"Wind\": (\n",
    "      ('NO_perturbation_Adaptation_OFF', 'Wind section', 95.0),\n",
    "      ('Static_0.7_Adaptation_ON', 'Wind section', 23.0),\n",
    "      ('Static_0.7_Adaptation_OFF', 'Wind section', 143.0)\n",
    "  )\n",
    "}\n",
    "chosen_robustness_trajectories = {\n",
    "  \"Ramp\": (\n",
    "      ('NO_perturbation_Adaptation_OFF', 'Ramp section', 23.0),\n",
    "      ('Static_0.7_Adaptation_ON', 'Ramp section', 61.0),\n",
    "      ('Static_0.7_Adaptation_OFF', 'Ramp section', 42.0),\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pidEQAxUbXx-",
   "metadata": {
    "id": "pidEQAxUbXx-"
   },
   "source": [
    "### Trajectories - Main trajectories plots function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a35xAxwI6I9-",
   "metadata": {
    "id": "a35xAxwI6I9-"
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(\n",
    "    dataframe,\n",
    "    section,\n",
    "    chosen_trajectories,\n",
    "    colormap,\n",
    "    adapt_idx,\n",
    "    adapt_name,\n",
    "    offset_name,\n",
    "    ax,\n",
    "    colorbar,\n",
    "):\n",
    "  # Getting data\n",
    "  trajectory = chosen_trajectories[section]\n",
    "\n",
    "  # Getting data\n",
    "  baseline = None\n",
    "  adapt = None\n",
    "  for name, group in dataframe:\n",
    "    if name == trajectory[0]:\n",
    "      baseline = group.copy()\n",
    "    if name == trajectory[adapt_idx]:\n",
    "      adapt = group.copy()\n",
    "  if baseline is None or adapt is None:\n",
    "    print(f\"Error: {trajectory[0]} or {trajectory[adapt_idx]} not in data\")\n",
    "    return\n",
    "\n",
    "  normalize = mcolors.Normalize(vmin=colormap[section][0], vmax=colormap[section][1])\n",
    "  cmap = sns.color_palette(\"viridis_r\", as_cmap=True)\n",
    "  sm = plt.cm.ScalarMappable(cmap=cmap, norm=normalize)\n",
    "  sm.set_array([])\n",
    "\n",
    "  # Trajectories\n",
    "  sns.scatterplot(data=baseline, x=\"tx\", y=\"ty\", marker='*', color='k', ax=ax)\n",
    "  sns.scatterplot(data=adapt, x=\"tx\", y=\"ty\", marker='x', s=100, c=cmap(normalize(adapt.timings.values)), ax=ax)\n",
    "  sns.scatterplot(data=baseline.iloc[[0]], x=\"tx\", y=\"ty\", marker='X', s=500, color='r', ax=ax)\n",
    "  sns.scatterplot(data=baseline.iloc[:-3], x=\"target_tx\", y=\"target_ty\", marker='X', s=125, color='k', ax=ax)\n",
    "\n",
    "  # Grid visual\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['left'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  ax.spines['bottom'].set_visible(False)\n",
    "  ax.grid(False)\n",
    "\n",
    "  # Title\n",
    "  ax.set_xlim(adapt[\"tx\"].min()-200, adapt[\"tx\"].max()+200)\n",
    "  ax.set_ylim(adapt[\"ty\"].min()-200, adapt[\"ty\"].max()+200)\n",
    "  ax.set_xlabel(\"\")\n",
    "  ax.set_xticklabels([])\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_ylabel(\"\")\n",
    "  ax.tick_params(axis='y', which='both', length=0.0)\n",
    "  ax.tick_params(axis='x', which='both', length=0.0)\n",
    "  if section_number == 0:\n",
    "    ax.text(offset_name, 0.5, adapt_name, transform=ax.transAxes, fontsize=fontsize, horizontalalignment='right', verticalalignment='bottom')\n",
    "\n",
    "  if section_number == 0 and not colorbar:\n",
    "    ax.text(-0.2, 1.0, \"C. Trajectories\", transform=ax.transAxes, fontsize=title_fontsize, weight=\"bold\", horizontalalignment='right', verticalalignment='bottom')\n",
    "\n",
    "  if colorbar:\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='horizontal')\n",
    "    cbar.set_label('Time between waypoints (s)')#, rotation=270)\n",
    "    cbar.ax.tick_params(length=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68dbe8e",
   "metadata": {},
   "source": [
    "### Trajectories - Formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3098d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_df = pd.concat([df.reset_index()[['Sections', 'Reps', 'Sections_index', 'targets', 'timings', 'target_tx', 'target_ty', 'tx', 'ty']], scaling_df.reset_index()[['Sections', 'Reps', 'Sections_index', 'targets', 'timings', 'target_tx', 'target_ty', 'tx', 'ty']], df_wind.reset_index()[['Sections', 'Reps', 'Sections_index', 'targets', 'timings', 'target_tx', 'target_ty', 'tx', 'ty']]])\n",
    "robustness_trajectory_df = robustness_df.reset_index()[['Sections', 'Reps', 'Sections_index', 'targets', 'timings', 'target_tx', 'target_ty', 'tx', 'ty']]\n",
    "\n",
    "trajectory_df = trajectory_df.groupby(['Reps', 'Sections', 'Sections_index'])[['targets', 'timings', 'target_tx', 'target_ty', 'tx', 'ty']]\n",
    "robustness_trajectory_df = robustness_trajectory_df.groupby(['Reps', 'Sections', 'Sections_index'])[['targets', 'timings', 'target_tx', 'target_ty', 'tx', 'ty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "k81rGX6iTJHc",
   "metadata": {
    "id": "k81rGX6iTJHc"
   },
   "outputs": [],
   "source": [
    "# Compute color map for each task\n",
    "colormap = {}\n",
    "for section in sections:\n",
    "\n",
    "  # Getting data\n",
    "  trajectory = chosen_trajectories[section]\n",
    "\n",
    "  # Getting data\n",
    "  for name, group in trajectory_df:\n",
    "    if name == trajectory[0]:\n",
    "      baseline = group.copy()\n",
    "    if name == trajectory[1]:\n",
    "      adapt_on = group.copy()\n",
    "    if name == trajectory[2]:\n",
    "      adapt_off = group.copy()\n",
    "  # baseline = trajectory_df.loc[trajectory[0]].copy()\n",
    "  # adapt_on = trajectory_df.loc[trajectory[1]].copy()\n",
    "  # adapt_off = trajectory_df.loc[trajectory[2]].copy()\n",
    "\n",
    "  # Getting min max\n",
    "  colormap_min = min(baseline.timings.min(), min(adapt_on.timings.min(), adapt_off.timings.min()))\n",
    "  colormap_max = max(baseline.timings.max(), max(adapt_on.timings.max(), adapt_off.timings.max()))\n",
    "  colormap[section] = [colormap_min, colormap_max]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb776e4",
   "metadata": {},
   "source": [
    "### Trajectories - Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3zOhKB4KSRDv",
   "metadata": {
    "id": "3zOhKB4KSRDv"
   },
   "outputs": [],
   "source": [
    "# Create figure\n",
    "figsize = (subfigure_width * 4, row_height * 10 * len(algorithms))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=figsize, gridspec_kw={'height_ratios': [1, 1.5]})\n",
    "\n",
    "# Ploting Adaptation On trajectory\n",
    "section_number = 0\n",
    "for section in sections:\n",
    "\n",
    "  ax = axes[0, section_number]\n",
    "  plot_trajectories(\n",
    "    dataframe=trajectory_df,\n",
    "    section=section,\n",
    "    chosen_trajectories=chosen_trajectories,\n",
    "    colormap=colormap,\n",
    "    adapt_idx=1,\n",
    "    adapt_name=flair,\n",
    "    offset_name=-0.2,\n",
    "    ax=ax,\n",
    "    colorbar=False,\n",
    "  )\n",
    "  section_number += 1\n",
    "\n",
    "section = \"Ramp\"\n",
    "ax = axes[0, section_number]\n",
    "plot_trajectories(\n",
    "  dataframe=robustness_trajectory_df,\n",
    "  section=section,\n",
    "  chosen_trajectories=chosen_robustness_trajectories,\n",
    "  colormap=colormap,\n",
    "  adapt_idx=1,\n",
    "  adapt_name='',\n",
    "  offset_name=-0.2,\n",
    "  ax=ax,\n",
    "  colorbar=False,\n",
    ")\n",
    "\n",
    "# Ploting Adaptation Off trajectory\n",
    "section_number = 0\n",
    "for section in sections:\n",
    "\n",
    "  ax = axes[1, section_number]\n",
    "  plot_trajectories(\n",
    "    dataframe=trajectory_df,\n",
    "    section=section,\n",
    "    chosen_trajectories=chosen_trajectories,\n",
    "    colormap=colormap,\n",
    "    adapt_idx=2,\n",
    "    adapt_name=no_flair,\n",
    "    offset_name=-0.2,\n",
    "    ax=ax,\n",
    "    colorbar=True,\n",
    "  )\n",
    "  section_number += 1\n",
    "\n",
    "section = \"Ramp\"\n",
    "ax = axes[1, section_number]\n",
    "plot_trajectories(\n",
    "  dataframe=robustness_trajectory_df,\n",
    "  section=section,\n",
    "  chosen_trajectories=chosen_robustness_trajectories,\n",
    "  colormap=colormap,\n",
    "  adapt_idx=2,\n",
    "  adapt_name='',\n",
    "  offset_name=-0.2,\n",
    "  ax=ax,\n",
    "  colorbar=True,\n",
    ")\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.05, left=0.0)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('./main_trajectories.png', bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gT7G52ruQFKF",
    "9nNlu1q9IcSZ",
    "9amLC4q3SswI",
    "mooyhIUktzsM",
    "Lhspqaeqx0T_",
    "09uV26sF208F",
    "Ymse1QkyGhRk",
    "Nk79tu3cAFsR",
    "TTF259HdmXm2",
    "Aha8NDgnVgCf",
    "svbf_hzsAiwk",
    "ZIVgmEi5W7gF",
    "lv8OK2TDW_o7",
    "vcEHNhxW4Rax",
    "78daFT73DgX9",
    "QXe_s7k4x96b",
    "pidEQAxUbXx-",
    "BgvZS0eqU9pm",
    "NcLfeo9TRgJs",
    "-ygE-VSwRaCs",
    "iWVF1jL9x_0i",
    "hNEgzemEebTJ",
    "YRcJ_uVHrPCP",
    "AMc974jNr0PC",
    "ydKcYfFdb9Fi",
    "VR2MZQxtGEBM",
    "ztKvtaBJyOQ-",
    "Ltp8kALBUe-o",
    "OO3cinQszRpL",
    "0eXMe21YzmD6",
    "w7R9TIXjS39E",
    "CxUeDR7lP_kp",
    "RcQJORZXPlpx",
    "Eccfllk0-63w",
    "rmNpM0xQxjy4",
    "c1bdc6c6",
    "yfg2lWAIc84r",
    "3387b97f",
    "d34965fb",
    "UcsAz3xGO9ms",
    "8fc2c2e0"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
